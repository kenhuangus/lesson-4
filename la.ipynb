{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stateful AI Agent for PII Scanning in Colab\n",
        "\n",
        "This notebook implements a stateful AI agent using LangGraph and Gemini to scan CSV files for Personally Identifiable Information (PII). The agent identifies PII, generates a report, and outputs a masked version of the CSV.\n",
        "\n",
        "**Agent Workflow:**\n",
        "1.  **Load CSV:** Ingests the source CSV file.\n",
        "2.  **Regex Scan:** Performs an initial pass using regular expressions to flag potential PII.\n",
        "3.  **LLM Classification:** Uses a Gemini model to analyze column names and data samples to adjudicate and enrich the findings.\n",
        "4.  **Consolidate:** Merges the findings from the regex and LLM steps.\n",
        "5.  **Mask & Save:** Applies masking rules to the identified PII columns and saves the masked CSV and a JSON report.\n",
        "6.  **Generate Report:** Creates a human-readable Markdown summary of the findings."
      ],
      "metadata": {
        "id": "J1h_Jg3rGz1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup and Installation\n",
        "\n",
        "This block installs the necessary Python libraries required to run the agent. We keep dependencies minimal as specified."
      ],
      "metadata": {
        "id": "vLd-jYj2I_WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph langchain langchain-google-genai pandas pydantic python-dotenv"
      ],
      "metadata": {
        "id": "e_b9643d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Configure API Key\n",
        "\n",
        "To use the Gemini model, you need a Google API key. \n",
        "\n",
        "**Instructions:**\n",
        "1.  Click on the **ðŸ”‘ (key) icon** in the left sidebar of Colab.\n",
        "2.  Click **\"Add a new secret\"**.\n",
        "3.  Name the secret `GOOGLE_API_KEY`.\n",
        "4.  Paste your API key into the \"Value\" field.\n",
        "5.  Make sure the \"Notebook access\" toggle is enabled.\n",
        "\n",
        "The code below will securely access this key. If the secret is not found, it will fall back to checking for an environment variable (useful for local development)."
      ],
      "metadata": {
        "id": "w3HwE_XkJIhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Attempt to get the key from Colab's user secrets\n",
        "try:\n",
        "    api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ['GOOGLE_API_KEY'] = api_key\n",
        "    print(\"Successfully loaded GOOGLE_API_KEY from Colab secrets.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Secret 'GOOGLE_API_KEY' not found in Colab secrets.\")\n",
        "    # Fallback for local execution if needed, though GOOGLE_API_KEY is preferred\n",
        "    if 'GEMINI_API_KEY' in os.environ:\n",
        "        print(\"Using GEMINI_API_KEY from environment variables.\")\n",
        "    elif 'GOOGLE_API_KEY' not in os.environ:\n",
        "        print(\"ERROR: Please set up the GOOGLE_API_KEY secret in Colab.\")\n",
        "    else:\n",
        "        print(\"Using GOOGLE_API_KEY from environment variables.\")\n"
      ],
      "metadata": {
        "id": "T0V0JvF_JdIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Imports and Pydantic Models\n",
        "\n",
        "This cell imports all the required libraries and defines the Pydantic models. These models provide structured data validation and settings for the agent's state (`AgentState`), configuration (`Config`), and output reports (`PIIReport`, `ColumnFinding`). Using Pydantic ensures that data flowing through the agent is well-defined and type-safe."
      ],
      "metadata": {
        "id": "3U7rXw_kKqYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import hashlib\n",
        "import json\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Literal, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# LangGraph / LangChain + Gemini\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.types import Stream\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# -----------------------------\n",
        "# Pydantic Models (State & I/O)\n",
        "# -----------------------------\n",
        "class PIISample(BaseModel):\n",
        "    column: str\n",
        "    row_index: int\n",
        "    raw_value: str\n",
        "    hashed_preview: str\n",
        "\n",
        "class ColumnFinding(BaseModel):\n",
        "    column: str\n",
        "    pii_types: List[str] = Field(default_factory=list)  # e.g., [\"EMAIL\", \"PHONE\"]\n",
        "    confidence: float = 0.0\n",
        "    rationale: Optional[str] = None\n",
        "    examples: List[PIISample] = Field(default_factory=list)\n",
        "\n",
        "class PIIReport(BaseModel):\n",
        "    columns_flagged: List[ColumnFinding] = Field(default_factory=list)\n",
        "    total_rows: int = 0\n",
        "    summary: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "class MaskRule(BaseModel):\n",
        "    pii_type: str  # \"EMAIL\", \"PHONE\", \"SSN\", \"CREDIT_CARD\", \"IP\", \"DOB\", \"ADDRESS\", \"NAME\"\n",
        "    strategy: Literal[\n",
        "        \"redact_all\",        # replace with ****\n",
        "        \"partial_email\",     # keep local first char & domain TLD\n",
        "        \"partial_phone\",     # keep last 4 digits\n",
        "        \"hash_consistent\",   # sha256 stable token\n",
        "        \"ipv4_subnet\",       # zero last octet\n",
        "        \"year_only\",         # for DOB\n",
        "    ]\n",
        "\n",
        "class Config(BaseModel):\n",
        "    sample_rows_for_llm: int = 8\n",
        "    sample_rows_for_regex: int = 200\n",
        "    max_examples_per_column: int = 5\n",
        "    mask_rules: List[MaskRule] = Field(default_factory=lambda: [\n",
        "        MaskRule(pii_type=\"EMAIL\", strategy=\"partial_email\"),\n",
        "        MaskRule(pii_type=\"PHONE\", strategy=\"partial_phone\"),\n",
        "        MaskRule(pii_type=\"SSN\", strategy=\"hash_consistent\"),\n",
        "        MaskRule(pii_type=\"CREDIT_CARD\", strategy=\"hash_consistent\"),\n",
        "        MaskRule(pii_type=\"IP\", strategy=\"ipv4_subnet\"),\n",
        "        MaskRule(pii_type=\"DOB\", strategy=\"year_only\"),\n",
        "        MaskRule(pii_type=\"ADDRESS\", strategy=\"redact_all\"),\n",
        "        MaskRule(pii_type=\"NAME\", strategy=\"redact_all\"),\n",
        "    ])\n",
        "    pii_regex: Dict[str, str] = Field(default_factory=lambda: {\n",
        "        \"EMAIL\": r\"(?i)\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\",\n",
        "        \"PHONE\": r\"(?:(?:\\+?1[-.\\s]?)?(?:\\(?\\d{3}\\)?|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4})\",\n",
        "        \"SSN\": r\"\\b\\d{3}-?\\d{2}-?\\d{4}\\b\",\n",
        "        \"CREDIT_CARD\": r\"\\b(?:\\d[ -]*?){13,16}\\b\",\n",
        "        \"IP\": r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\",\n",
        "        \"DOB\": r\"\\b(?:\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\\b\",\n",
        "        # ADDRESS/NAME are hard via regex; handled by LLM hints mostly\n",
        "    })\n",
        "\n",
        "class AgentState(BaseModel):\n",
        "    # Inputs\n",
        "    input_csv: str\n",
        "    outdir: str\n",
        "    model: str = \"gemini-1.5-pro\"\n",
        "    config: Config = Field(default_factory=Config)\n",
        "\n",
        "    # Working data\n",
        "    df_head: Optional[List[Dict[str, Any]]] = None\n",
        "    columns: List[str] = Field(default_factory=list)\n",
        "    llm_column_analysis: List[ColumnFinding] = Field(default_factory=list)\n",
        "    regex_column_analysis: List[ColumnFinding] = Field(default_factory=list)\n",
        "\n",
        "    # Outputs\n",
        "    report: Optional[PIIReport] = None\n",
        "    masked_csv_path: Optional[str] = None\n",
        "    findings_json_path: Optional[str] = None\n",
        "    report_md_path: Optional[str] = None\n",
        "\n",
        "    # Meta\n",
        "    total_rows: int = 0\n",
        "    errors: List[str] = Field(default_factory=list)\n",
        "    logs: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "7L9l-5_QKx5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Utility Functions\n",
        "\n",
        "These are helper functions used across different nodes of the agent. They handle tasks like creating secure hashes of data (for privacy when sending samples to the LLM) and ensuring the output directory exists."
      ],
      "metadata": {
        "id": "1VdY0t3rLCCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sha256_token(v: str) -> str:\n",
        "    return hashlib.sha256(v.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:12]\n",
        "\n",
        "\n",
        "def hashed_preview(v: Any) -> str:\n",
        "    s = str(v)[:64]\n",
        "    return sha256_token(s)\n",
        "\n",
        "\n",
        "def ensure_outdir(path: str) -> Path:\n",
        "    p = Path(path)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p"
      ],
      "metadata": {
        "id": "67v8l4YqLJm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. LangGraph Agent Nodes\n",
        "\n",
        "Each function below represents a single node in our stateful graph. A node is a unit of work that receives the current `AgentState`, performs a task, and returns the updated state. We break down the agent's logic into these modular, testable, and reusable components."
      ],
      "metadata": {
        "id": "Nq_D071uLPeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node 1: `node_load_csv`\n",
        "\n",
        "This is the entry point of the graph. It reads the CSV file specified in the initial state, extracts metadata like row count and column names, and stores a small sample of the data in the state for later use. It also attaches the full DataFrame to a temporary attribute `_df_cache` for efficient access by subsequent nodes, avoiding the need to pass the entire dataset through the state's main fields."
      ],
      "metadata": {
        "id": "jD6k74f0LSfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_load_csv(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        df = pd.read_csv(state.input_csv)\n",
        "        state.total_rows = len(df)\n",
        "        state.columns = list(df.columns)\n",
        "        # store a small head snapshot (not full data) for reporting\n",
        "        state.df_head = df.head(10).to_dict(orient=\"records\")\n",
        "        state.logs.append(f\"Loaded CSV with {state.total_rows} rows and {len(state.columns)} columns.\")\n",
        "        state._df_cache = df  # type: ignore[attr-defined]\n",
        "    except Exception as e:\n",
        "        state.errors.append(f\"load_csv: {e}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "GvJ3K5yHLg1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node 2: `node_regex_scan`\n",
        "\n",
        "This node performs a fast, preliminary scan for PII using a predefined set of regular expressions. It operates on a sample of the data to remain efficient. For each column, it counts matches for different PII types (e.g., EMAIL, PHONE) and collects a few examples. This provides a baseline set of findings that can be used to guide the more sophisticated LLM analysis in the next step."
      ],
      "metadata": {
        "id": "J1x37e2zLmgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_regex_scan(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        df = getattr(state, \"_df_cache\")  # type: ignore[attr-defined]\n",
        "        findings: List[ColumnFinding] = []\n",
        "        sample_df = df.head(state.config.sample_rows_for_regex)\n",
        "        for col in state.columns:\n",
        "            col_find = ColumnFinding(column=col)\n",
        "            series = sample_df[col].astype(str).fillna(\"\")\n",
        "            matches: Dict[str, int] = {}\n",
        "            examples: List[PIISample] = []\n",
        "            for pii_type, pattern in state.config.pii_regex.items():\n",
        "                count = series.str.contains(pattern, regex=True).sum()\n",
        "                if count > 0:\n",
        "                    matches[pii_type] = int(count)\n",
        "            # collect a few examples\n",
        "            if matches:\n",
        "                for idx, val in series.items():\n",
        "                    for pii_type, pattern in state.config.pii_regex.items():\n",
        "                        if series.loc[idx] and pd.notna(series.loc[idx]) and pd.notna(val):\n",
        "                            if pd.Series([str(val)]).str.contains(pattern, regex=True).iloc[0]:\n",
        "                                examples.append(\n",
        "                                    PIISample(\n",
        "                                        column=col,\n",
        "                                        row_index=int(idx),\n",
        "                                        raw_value=str(val)[:64],\n",
        "                                        hashed_preview=hashed_preview(val),\n",
        "                                    )\n",
        "                                )\n",
        "                                break\n",
        "                col_find.pii_types = sorted(matches.keys())\n",
        "                # simple confidence heuristic\n",
        "                col_find.confidence = min(1.0, sum(matches.values()) / max(1, state.config.sample_rows_for_regex))\n",
        "                col_find.rationale = \"Regex/heuristic match counts over sample\"\n",
        "                col_find.examples = examples[: state.config.max_examples_per_column]\n",
        "            findings.append(col_find)\n",
        "        state.regex_column_analysis = findings\n",
        "        state.logs.append(\"Completed regex scan.\")\n",
        "    except Exception as e:\n",
        "        state.errors.append(f\"regex_scan: {e}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "aL-eX8x9Lq9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node 3: `node_llm_classify`\n",
        "\n",
        "This node leverages a Gemini LLM to perform a more nuanced analysis. It sends the column names, the hints from the regex scan, and a small number of *hashed* data samples to the model. By sending only hashed previews, we protect the raw data from being exposed to the LLM. The LLM is prompted to act as a security specialist and return a structured JSON response identifying PII types, its confidence, and a rationale for its decision for each column."
      ],
      "metadata": {
        "id": "e3T_u23PLw-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_llm_classify(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        # Configure Gemini (prefers GOOGLE_API_KEY)\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Missing GOOGLE_API_KEY. Please configure it in Colab secrets.\")\n",
        "        llm = ChatGoogleGenerativeAI(model=state.model, google_api_key=api_key, temperature=0.1)\n",
        "\n",
        "        # Prepare redacted samples per column (hash previews only)\n",
        "        sample_rows = []\n",
        "        df = getattr(state, \"_df_cache\")  # type: ignore[attr-defined]\n",
        "        head = df.head(state.config.sample_rows_for_llm)\n",
        "        for i, row in head.iterrows():\n",
        "            sample_rows.append({c: hashed_preview(row[c]) for c in state.columns})\n",
        "\n",
        "        system = SystemMessage(\n",
        "            content=(\n",
        "                \"You are a Senior Security Engineer specializing in data privacy and compliance.\\n\"\n",
        "                \"Task: Given CSV schema and hashed examples, identify which columns likely contain PII.\\n\"\n",
        "                \"Return ONLY a JSON array of objects: {column, pii_types:[...], confidence:0..1, rationale}.\\n\"\n",
        "                \"PII types limited to: EMAIL, PHONE, SSN, CREDIT_CARD, IP, DOB, ADDRESS, NAME, NONE.\\n\"\n",
        "                \"Prefer precision over recall; do not hallucinate.\\n\"\n",
        "            )\n",
        "        )\n",
        "        human = HumanMessage(\n",
        "            content=json.dumps({\n",
        "                \"columns\": state.columns,\n",
        "                \"regex_hints\": [cf.model_dump() for cf in state.regex_column_analysis],\n",
        "                \"hashed_samples\": sample_rows,\n",
        "            })\n",
        "        )\n",
        "        resp = llm.invoke([system, human])\n",
        "        text = resp.content if isinstance(resp, AIMessage) else str(resp)\n",
        "        parsed: List[Dict[str, Any]] = []\n",
        "        try:\n",
        "            # Clean the text to extract only the JSON part\n",
        "            json_text = text.strip().replace('`json', '').replace('`', '')\n",
        "            parsed = json.loads(json_text)  # expect array\n",
        "        except Exception:\n",
        "            # attempt to extract JSON block with regex as a fallback\n",
        "            m = re.search(r\"\\\\[\\\\s*{[\\\\s\\\\S]*}\\\\s*\\\\]\", text)\n",
        "            if m:\n",
        "                parsed = json.loads(m.group(0))\n",
        "            else:\n",
        "                raise\n",
        "        findings = []\n",
        "        for item in parsed:\n",
        "            findings.append(\n",
        "                ColumnFinding(\n",
        "                    column=item.get(\"column\", \"\"),\n",
        "                    pii_types=[pt for pt in item.get(\"pii_types\", []) if pt != \"NONE\"],\n",
        "                    confidence=float(item.get(\"confidence\", 0.0)),\n",
        "                    rationale=item.get(\"rationale\"),\n",
        "                )\n",
        "            )\n",
        "        state.llm_column_analysis = findings\n",
        "        state.logs.append(\"LLM classification completed.\")\n",
        "    except Exception as e:\n",
        "        state.errors.append(f\"llm_classify: {e}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "zVf12_W9L0uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node 4: `node_consolidate`\n",
        "\n",
        "This node merges the results from the regex scan and the LLM classification. It combines the identified PII types for each column, taking the highest confidence score and appending the rationales. This creates a single, unified list of findings. It then populates the main `PIIReport` in the state with these consolidated findings and a summary of PII types found."
      ],
      "metadata": {
        "id": "e_zB1U9QL8qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _merge_findings(regex: List[ColumnFinding], llm: List[ColumnFinding]) -> List[ColumnFinding]:\n",
        "    by_col: Dict[str, ColumnFinding] = {c.column: c for c in regex}\n",
        "    for lf in llm:\n",
        "        if lf.column in by_col:\n",
        "            base = by_col[lf.column]\n",
        "            merged_types = sorted(set(base.pii_types) | set(lf.pii_types))\n",
        "            base.pii_types = merged_types\n",
        "            base.confidence = max(base.confidence, lf.confidence)\n",
        "            base.rationale = (base.rationale or \"\") + \" | LLM: \" + (lf.rationale or \"\")\n",
        "        else:\n",
        "            by_col[lf.column] = lf\n",
        "    return list(by_col.values())\n",
        "\n",
        "\n",
        "def node_consolidate(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        cols = _merge_findings(state.regex_column_analysis, state.llm_column_analysis)\n",
        "        # Filter out columns with no detected PII types\n",
        "        flagged_cols = [c for c in cols if c.pii_types]\n",
        "        report = PIIReport(columns_flagged=flagged_cols, total_rows=state.total_rows)\n",
        "        # summary counts by pii type\n",
        "        summary: Dict[str, int] = {}\n",
        "        for c in flagged_cols:\n",
        "            for t in c.pii_types:\n",
        "                summary[t] = summary.get(t, 0) + 1\n",
        "        report.summary = {\"columns_with_type\": summary}\n",
        "        state.report = report\n",
        "        state.logs.append(\"Consolidated findings.\")\n",
        "    except Exception as e:\n",
        "        state.errors.append(f\"consolidate: {e}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "4eUf6z15MB3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node 5: `node_mask_and_save`\n",
        "\n",
        "This node takes the final, consolidated report and applies the configured masking rules to the original data. It iterates through the columns identified as containing PII and applies the appropriate masking strategy (e.g., redacting, hashing, or partial replacement). The resulting masked DataFrame is then written to `masked.csv`, and the detailed findings are saved to `findings.json` in the specified output directory."
      ],
      "metadata": {
        "id": "eJj9M9s1MGX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mask(value: Any, pii_types: List[str], rules: List[MaskRule]) -> str:\n",
        "    s = str(value)\n",
        "    if not s or pd.isna(value) or not pii_types:\n",
        "        return s\n",
        "    # apply first matching rule by preference order\n",
        "    for t in pii_types:\n",
        "        rule = next((r for r in rules if r.pii_type == t), None)\n",
        "        if not rule:\n",
        "            continue\n",
        "        strat = rule.strategy\n",
        "        if strat == \"redact_all\":\n",
        "            return \"â–ˆâ–ˆâ–ˆâ–ˆ\"\n",
        "        if strat == \"hash_consistent\":\n",
        "            return f\"token_{sha256_token(s)}\"\n",
        "        if strat == \"partial_email\":\n",
        "            parts = s.split(\"@\")\n",
        "            if len(parts) == 2:\n",
        "                local, domain = parts\n",
        "                tld = domain.split(\".\")[-1] if \".\" in domain else \"dom\"\n",
        "                return f\"{local[:1]}***@***.{tld}\"\n",
        "            return f\"token_{sha256_token(s)}\"\n",
        "        if strat == \"partial_phone\":\n",
        "            digits = ''.join(ch for ch in s if ch.isdigit())\n",
        "            return f\"***-***-{digits[-4:]}\" if len(digits) >= 4 else \"***-***-****\"\n",
        "        if strat == \"ipv4_subnet\":\n",
        "            parts = s.split('.')\n",
        "            if len(parts) == 4:\n",
        "                return '.'.join(parts[:3] + ['0'])\n",
        "            return f\"ip_{sha256_token(s)}\"\n",
        "        if strat == \"year_only\":\n",
        "            m = re.search(r\"(\\d{4})\", s)\n",
        "            return m.group(1) if m else \"YYYY\"\n",
        "    # fallback\n",
        "    return f\"token_{sha256_token(s)}\"\n",
        "\n",
        "\n",
        "def node_mask_and_save(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        df = getattr(state, \"_df_cache\")  # type: ignore[attr-defined]\n",
        "        # Build map column -> pii_types\n",
        "        pii_map: Dict[str, List[str]] = {c.column: c.pii_types for c in state.report.columns_flagged} if state.report else {}\n",
        "        rules = state.config.mask_rules\n",
        "        masked = df.copy()\n",
        "        for col, types in pii_map.items():\n",
        "            if not types:\n",
        "                continue\n",
        "            masked[col] = masked[col].apply(lambda v: apply_mask(v, types, rules))\n",
        "        outdir = ensure_outdir(state.outdir)\n",
        "        masked_path = outdir / \"masked.csv\"\n",
        "        masked.to_csv(masked_path, index=False)\n",
        "        state.masked_csv_path = str(masked_path)\n",
        "        # Write findings.json\n",
        "        findings_json = outdir / \"findings.json\"\n",
        "        with open(findings_json, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(state.report.model_dump() if state.report else {}, f, indent=2)\n",
        "        state.findings_json_path = str(findings_json)\n",
        "        state.logs.append(f\"Masked CSV saved to {masked_path}\")\n",
        "    except Exception as e:\n",
        "        state.errors.append(f\"mask_and_save: {e}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "e_bU0M_nMLlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Node 6: `node_generate_report`\n",
        "\n",
        "This is the final node in the workflow. It creates a human-readable summary of the agent's findings in Markdown format. The report includes a summary of PII types discovered, details for each flagged column (including PII types, confidence, and hashed examples), and file paths. This report is saved as `report.md`."
      ],
      "metadata": {
        "id": "1m0oQvCMMTCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_generate_report(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        outdir = ensure_outdir(state.outdir)\n",
        "        md_path = outdir / \"report.md\"\n",
        "        report = state.report or PIIReport()\n",
        "        lines = [\n",
        "            \"# PII Detection Report\",\n",
        "            \"\",\n",
        "            f\"**Input:** `{state.input_csv}`\",\n",
        "            f\"**Rows:** {state.total_rows}\",\n",
        "            \"\",\n",
        "            \"## Summary\",\n",
        "            \"```json\",\n",
        "            json.dumps(report.summary, indent=2),\n",
        "            \"```\",\n",
        "            \"\",\n",
        "            \"## Flagged Columns Details\",\n",
        "        ]\n",
        "        if not report.columns_flagged:\n",
        "            lines.append(\"No PII was detected in any columns.\")\n",
        "        else:\n",
        "            for c in report.columns_flagged:\n",
        "                lines += [\n",
        "                    f\"### Column: `{c.column}`\",\n",
        "                    f\"- **PII Types**: {', '.join(c.pii_types) if c.pii_types else 'None'}\",\n",
        "                    f\"- **Confidence**: {c.confidence:.2f}\",\n",
        "                    f\"- **Rationale**: {c.rationale or '-'}\",\n",
        "                ]\n",
        "                if c.examples:\n",
        "                    lines.append(\"- **Examples (hashed previews):**\")\n",
        "                    for ex in c.examples[:3]:\n",
        "                        lines.append(f\"  - `row {ex.row_index}`: {ex.hashed_preview}\")\n",
        "                lines.append(\"\")\n",
        "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(lines))\n",
        "        state.report_md_path = str(md_path)\n",
        "        state.logs.append(f\"Report written to {md_path}\")\n",
        "    except Exception as e:\n",
        "        state.errors.append(f\"generate_report: {e}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "e_z4k8yMMY1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Build and Compile the Graph\n",
        "\n",
        "This function assembles all the individual nodes into a coherent workflow using `StateGraph`. It defines the sequence of operations by adding edges between the nodes, creating a linear progression from loading the CSV to generating the final report. Finally, it compiles the graph into a runnable application."
      ],
      "metadata": {
        "id": "uO9-858_McpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph() -> StateGraph:\n",
        "    graph = StateGraph(AgentState)\n",
        "    graph.add_node(\"load_csv\", node_load_csv)\n",
        "    graph.add_node(\"regex_scan\", node_regex_scan)\n",
        "    graph.add_node(\"llm_classify\", node_llm_classify)\n",
        "    graph.add_node(\"consolidate\", node_consolidate)\n",
        "    graph.add_node(\"mask_and_save\", node_mask_and_save)\n",
        "    graph.add_node(\"generate_report\", node_generate_report)\n",
        "\n",
        "    graph.set_entry_point(\"load_csv\")\n",
        "    graph.add_edge(\"load_csv\", \"regex_scan\")\n",
        "    graph.add_edge(\"regex_scan\", \"llm_classify\")\n",
        "    graph.add_edge(\"llm_classify\", \"consolidate\")\n",
        "    graph.add_edge(\"consolidate\", \"mask_and_save\")\n",
        "    graph.add_edge(\"mask_and_save\", \"generate_report\")\n",
        "    graph.add_edge(\"generate_report\", END)\n",
        "    return graph"
      ],
      "metadata": {
        "id": "l_GqQkG5Mfu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Prepare Data and Define Inputs\n",
        "\n",
        "This cell creates a sample `customers.csv` file in the Colab environment to make the notebook self-contained and runnable without requiring a file upload. It also defines the input and output directories that the agent will use."
      ],
      "metadata": {
        "id": "kU3D1sVIMmXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy data directory and a sample CSV file\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "os.makedirs(\"./out\", exist_ok=True)\n",
        "\n",
        "csv_content = \"\"\"\n",
        "name,email,phone,notes,ip_address\n",
        "Alice,alice@example.com,+1-415-555-0199,Call after 5pm,192.168.1.10\n",
        "Bob,bob@acme.co,4155550188,VIP Customer,203.0.113.45\n",
        "Charlie,charlie+test@gmail.com,555-867-5309,Met at conference,198.51.100.2\n",
        "Diana,diana@work.net,(415) 555-0122,Follow up next week,2001:db8:85a3:8d3:1319:8a2e:370:7348\n",
        "\"\"\"\n",
        "\n",
        "input_csv_path = \"./data/customers.csv\"\n",
        "with open(input_csv_path, \"w\") as f:\n",
        "    f.write(csv_content.strip())\n",
        "\n",
        "print(f\"Sample CSV created at: {input_csv_path}\")\n",
        "\n",
        "# Define agent inputs\n",
        "INPUT_CSV = input_csv_path\n",
        "OUTPUT_DIR = \"./out\"\n",
        "MODEL_NAME = \"gemini-1.5-pro-latest\" # Using the latest 1.5 Pro model"
      ],
      "metadata": {
        "id": "uO6_k7N5MqPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Run the Agent\n",
        "\n",
        "This is the main execution block. It initializes the `AgentState` with the input parameters, compiles the graph, and then runs the agent. We use `app.stream` to execute the graph, which allows us to see logs and errors from each node in real-time as it completes its task. After the run, a final summary of the output file paths is printed."
      ],
      "metadata": {
        "id": "5YJpY39iMyqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the state with our inputs\n",
        "initial_state = AgentState(\n",
        "    input_csv=INPUT_CSV, \n",
        "    outdir=OUTPUT_DIR, \n",
        "    model=MODEL_NAME\n",
        ")\n",
        "\n",
        "# Compile the graph\n",
        "app = build_graph().compile()\n",
        "\n",
        "# Stream execution (yields state updates per node)\n",
        "print(\"--- Running PII Agent ---\")\n",
        "final_state = {}\n",
        "for update in app.stream(initial_state, stream_mode=\"values\"):\n",
        "    if isinstance(update, AgentState):\n",
        "        if update.logs:\n",
        "            print(f\"LOG: {update.logs[-1]}\")\n",
        "        if update.errors:\n",
        "            print(f\"ERROR: {update.errors[-1]}\")\n",
        "        final_state = update\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n=== Agent Run Complete ===\")\n",
        "print(f\"Masked CSV:      {final_state.masked_csv_path}\")\n",
        "print(f\"Findings JSON:   {final_state.findings_json_path}\")\n",
        "print(f\"Report MD:       {final_state.report_md_path}\")\n",
        "if final_state.errors:\n",
        "    print(f\"Errors encountered: {final_state.errors}\")"
      ],
      "metadata": {
        "id": "2L9X8nS-M5E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Review Outputs\n",
        "\n",
        "The following cells display the contents of the files generated by the agent, allowing for immediate verification of the results."
      ],
      "metadata": {
        "id": "e_z24xV8M-bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masked CSV (`masked.csv`)"
      ],
      "metadata": {
        "id": "xGq3BvTjNFw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_df = pd.read_csv(final_state.masked_csv_path)\n",
        "print(masked_df.to_markdown(index=False))"
      ],
      "metadata": {
        "id": "Wz1hI6rDNIdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Findings JSON (`findings.json`)"
      ],
      "metadata": {
        "id": "hU_IasFCNNCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(final_state.findings_json_path, 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "f51H3_vKNN2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Markdown Report (`report.md`)"
      ],
      "metadata": {
        "id": "3U7rXw_kNQyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "with open(final_state.report_md_path, 'r') as f:\n",
        "    md_content = f.read()\n",
        "    \n",
        "display(Markdown(md_content))"
      ],
      "metadata": {
        "id": "2L9X8nS-NScv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Assessment & Suggestions\n",
        "\n",
        "-   **Security**: We avoid sending raw values to the LLM; only hashed previews + regex hints are shared.\n",
        "-   **Performance**: Regex scan is bounded to N rows; consider sampling strategies for very large CSVs.\n",
        "-   **Extensibility**: Add custom rules per jurisdiction (GDPR special categories). Add entity-level masking (named entities inside free text) with a local spaCy NER fallback if LLM is not allowed.\n",
        "-   **Reliability**: Add unit tests with synthetic data. Optionally enable forced function calling via Vertex AI tool config when you migrate to Vertex.\n",
        "-   **Observability**: Persist `state.logs` and timings; integrate LangSmith or OpenTelemetry as needed."
      ],
      "metadata": {
        "id": "x-K2c2WPNY1e"
      }
    }
  ]
}